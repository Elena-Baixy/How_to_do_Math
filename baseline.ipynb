{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wLEDNibkrcNa"
      },
      "outputs": [],
      "source": [
        "!pip install transformers datasets torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt: Problem + options\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"math_qa\", split=\"validation\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2-medium', num_labels=5)\n",
        "model.eval()\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def predict(text):\n",
        "    # inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=tokenizer.model_max_length)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return torch.argmax(logits, dim=1)\n",
        "answer_mapping = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}\n",
        "\n",
        "correct = 0\n",
        "num_example = 100\n",
        "for i in range(num_example):\n",
        "    prompt = dataset[i][\"Problem\"] + \" \" + dataset[i][\"options\"]\n",
        "    prediction_index = predict(prompt).item()\n",
        "    prediction_label = answer_mapping[prediction_index]\n",
        "    # print(prediction_label)\n",
        "    correct_answer = dataset[i]['correct']\n",
        "    if prediction_label == correct_answer:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / num_example\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YZdz0cTArt-9",
        "outputId": "7ffbb3a9-259a-4aed-ba3f-ef2e35b86d01"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt: Problem + Rationale + Options\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"math_qa\", split=\"validation\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2-medium', num_labels=5)\n",
        "model.eval()\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def predict(text):\n",
        "    # inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=tokenizer.model_max_length)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return torch.argmax(logits, dim=1)\n",
        "answer_mapping = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}\n",
        "\n",
        "correct = 0\n",
        "num_example = 100\n",
        "for i in range(num_example):\n",
        "    prompt = dataset[i][\"Problem\"] + \" \" + dataset[i][\"Rationale\"] + \" \" + dataset[i][\"options\"]\n",
        "    prediction_index = predict(prompt).item()\n",
        "    prediction_label = answer_mapping[prediction_index]\n",
        "    # print(prediction_label)\n",
        "    correct_answer = dataset[i]['correct']\n",
        "    if prediction_label == correct_answer:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / num_example\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rbBqUPgs7HA",
        "outputId": "99f89e0e-a4a6-4005-9d3d-322f39fc9a55"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt: Annotated_formula + Options\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"math_qa\", split=\"validation\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2-medium', num_labels=5)\n",
        "model.eval()\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def predict(text):\n",
        "    # inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=tokenizer.model_max_length)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return torch.argmax(logits, dim=1)\n",
        "answer_mapping = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}\n",
        "\n",
        "correct = 0\n",
        "num_example = 100\n",
        "for i in range(num_example):\n",
        "    prompt = dataset[i][\"annotated_formula\"] + \" \" + dataset[i][\"options\"]\n",
        "    prediction_index = predict(prompt).item()\n",
        "    prediction_label = answer_mapping[prediction_index]\n",
        "    # print(prediction_label)\n",
        "    correct_answer = dataset[i]['correct']\n",
        "    if prediction_label == correct_answer:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / num_example\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOFDCIrZvkiz",
        "outputId": "34014354-1252-486a-df10-584f2d593d3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt: Problem + Annotated_formla + options\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"math_qa\", split=\"validation\")\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
        "model = GPT2ForSequenceClassification.from_pretrained('gpt2-medium', num_labels=5)\n",
        "model.eval()\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "def predict(text):\n",
        "    # inputs = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=tokenizer.model_max_length)\n",
        "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    return torch.argmax(logits, dim=1)\n",
        "answer_mapping = {0: \"a\", 1: \"b\", 2: \"c\", 3: \"d\", 4: \"e\"}\n",
        "\n",
        "correct = 0\n",
        "num_example = 100\n",
        "for i in range(num_example):\n",
        "    prompt = dataset[i][\"Problem\"] + \" \" + dataset[i][\"annotated_formula\"] + \" \" + dataset[i][\"options\"]\n",
        "    prediction_index = predict(prompt).item()\n",
        "    prediction_label = answer_mapping[prediction_index]\n",
        "    # print(prediction_label)\n",
        "    correct_answer = dataset[i]['correct']\n",
        "    if prediction_label == correct_answer:\n",
        "        correct += 1\n",
        "\n",
        "accuracy = correct / num_example\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrM10ADPvyll",
        "outputId": "872c8ac9-21a6-4422-f673-e5fd554b6048"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2-medium and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.23\n"
          ]
        }
      ]
    }
  ]
}